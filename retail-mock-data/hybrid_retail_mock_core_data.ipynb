{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0fa7ae-51b3-4104-ba3d-532195a15bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Importing necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98fc40b-4738-4330-865c-7eb6e1da7274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T, Window\n",
    "from datetime import datetime, timedelta\n",
    "import uuid, random, json\n",
    "import random as _r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25f6d46c-c138-4d33-ab38-d98a5c944c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Preparing dataspace in UC ( unity catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83e0cdd-ad0a-4eb6-9b64-a9f726669ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--create catalog if not exists workspace; --managed location ;\n",
    "create schema  if not exists workspace.files comment 'test schema';\n",
    "create volume  if not exists  workspace.files.testbed COMMENT 'Testbed for sample data';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5259f449-ae1d-4f25-a3c8-f1d9644dedf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setting up path\n",
    "Before filling it up create volume in catalog and copy base path from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7caadab7-0d5d-4284-b821-7d78bc954ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "BRONZE_ROOT = f\"/Volumes/workspace/files/testbed/hybrid_retail/bronze\"\n",
    "LANDING_ROOT = f\"/Volumes/workspace/files/testbed/hybrid_retail/landing\"\n",
    "\n",
    "PATHS = {\n",
    "    \"customers\": f\"{BRONZE_ROOT}/customers\",\n",
    "    \"products\":  f\"{BRONZE_ROOT}/products\",\n",
    "    \"stores\":    f\"{BRONZE_ROOT}/stores\",\n",
    "    \"employees\": f\"{BRONZE_ROOT}/employees\",\n",
    "    \"inventory\": f\"{BRONZE_ROOT}/inventory\",\n",
    "    \"sales\":     f\"{BRONZE_ROOT}/sales\",\n",
    "    \"api_products_updates\": f\"{LANDING_ROOT}/api_products_updates_json\",\n",
    "    \"sftp_inventory\":       f\"{LANDING_ROOT}/sftp_inventory_csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ddd76f1-2b04-46b5-8a09-424616bf5768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Required Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a45cd0-22c7-409b-9fa2-7e76b374dc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for p in PATHS.values():\n",
    "    dbutils.fs.mkdirs(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eafff2dc-2427-4157-8140-89b8c31cb7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Adding Guiding Elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1e2041-b27d-403d-a65e-35e28aa1ee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ODD_NAMES = [\"Müller\",\"José\",\"Zoë\",\"François\",\"IKEA – Malmö\",\"Café Au Lait\",\"Pokémon\",\"Naïve\",\"Smörgås\"]\n",
    "CURRENCIES = [\"SEK\",\"EUR\",\"USD\",\"GBP\"]\n",
    "COUNTRIES = [\"SE\",\"DE\",\"FR\",\"NL\",\"GB\",\"US\"]\n",
    "REGIONS = [\"Nordics\",\"DACH\",\"Benelux\",\"UKI\",\"North America\"]\n",
    "CATEGORIES = [\"Electronics\",\"Home Office\",\"Audio\",\"Accessories\",\"Gaming\",\"Appliances\"]\n",
    "BRANDS = [\"Contoso\",\"Fabrikam\",\"Litware\",\"Northwind\",\"AdventureWorks\",\"Tailspin\"]\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7d62767-9ac8-4d65-8cbc-ddeedb7252f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb5d478-24d6-436c-8514-87d9f55c3195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def new_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def random_ts_days_ago(max_days=365):\n",
    "    return (datetime.utcnow() - timedelta(days=random.randint(0, max_days)))\n",
    "\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def udf_random_currency():\n",
    "    import random\n",
    "    return random.choice(CURRENCIES)\n",
    "\n",
    "@F.udf(\"double\")\n",
    "def udf_price_noise(x):\n",
    "    import random\n",
    "    def monetary_noise(amount: float):\n",
    "        r = random.random()\n",
    "        if r < 0.01:\n",
    "            return round(amount * random.uniform(10, 40), 2)\n",
    "        if r < 0.03:\n",
    "            return round(amount * random.uniform(0.01, 0.25), 2)\n",
    "        return round(amount, 2)\n",
    "    return monetary_noise(x if x else 0.0)\n",
    "\n",
    "\n",
    "def inject_encoding(s):\n",
    "    return _r.choice(ODD_NAMES) if _r.random() < 0.05 else s\n",
    "\n",
    "def maybe_null(v, p=0.03):\n",
    "    return None if _r.random() < p else v\n",
    "\n",
    "def maybe_empty(v, p=0.02):\n",
    "    return \"\" if _r.random() < p else v\n",
    "\n",
    "def maybe_inconsistent_date(ts, p=0.05):\n",
    "    if _r.random() > p:\n",
    "        return ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    choices = [ts.strftime(\"%d/%m/%Y\"), ts.strftime(\"%m-%d-%Y\"), ts.isoformat(timespec=\"seconds\"), \"2025-13-40\"]\n",
    "    return _r.choice(choices)\n",
    "\n",
    "# ---- Dimensions\n",
    "\n",
    "def gen_customers(n):\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        cid = new_uuid()\n",
    "        created_at = random_ts_days_ago(900)\n",
    "        last_updated = created_at\n",
    "        is_active = 1 if _r.random() > 0.05 else 0\n",
    "        first = _r.choice([\"Alice\",\"Bob\",\"Charlie\",\"Diana\",\"Erik\",\"Fatima\",\"Göran\",\"Hanna\",\"Ivan\",\"Julia\",\"Karl\"])\n",
    "        last  = _r.choice([\"Andersson\",\"Berg\",\"Carlsson\",\"Dahl\",\"Ekström\",\"Fischer\",\"Ghosh\",\"Hernández\",\"Ilyas\",\"Johansson\"])\n",
    "        full  = inject_encoding(f\"{first} {last}\")\n",
    "        email = maybe_empty(maybe_null(f\"{first.lower()}.{last.lower()}@example.com\"))\n",
    "        phone = maybe_null(f\"+46{_r.randint(700000000, 799999999)}\")\n",
    "        birth = random_ts_days_ago(365*60)\n",
    "        country = _r.choice(COUNTRIES)\n",
    "        currency = _r.choice(CURRENCIES)\n",
    "        rows.append((cid, full, email, phone, maybe_inconsistent_date(birth), country, currency, is_active, maybe_inconsistent_date(last_updated), maybe_inconsistent_date(created_at)))\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"customer_id\", T.StringType(), False),\n",
    "        T.StructField(\"full_name\", T.StringType(), True),\n",
    "        T.StructField(\"email\", T.StringType(), True),\n",
    "        T.StructField(\"phone\", T.StringType(), True),\n",
    "        T.StructField(\"birth_date\", T.StringType(), True),\n",
    "        T.StructField(\"country_code\", T.StringType(), True),\n",
    "        T.StructField(\"preferred_currency\", T.StringType(), True),\n",
    "        T.StructField(\"is_active\", T.IntegerType(), True),\n",
    "        T.StructField(\"last_updated\", T.StringType(), True),\n",
    "        T.StructField(\"created_at\", T.StringType(), True),\n",
    "    ])\n",
    "    return spark.createDataFrame(rows, schema)\n",
    "\n",
    "def gen_stores(n):\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        sid = new_uuid()\n",
    "        created_at = random_ts_days_ago(1200)\n",
    "        last_updated = created_at\n",
    "        is_active = 1 if _r.random() > 0.08 else 0\n",
    "        name = inject_encoding(f\"Store-{i+1:03d}\")\n",
    "        region = _r.choice(REGIONS)\n",
    "        city = _r.choice([\"Malmö\",\"Stockholm\",\"Göteborg\",\"Lund\",\"Helsingborg\",\"Uppsala\",\"Copenhagen\",\"Hamburg\",\"Amsterdam\",\"London\",\"Seattle\"])\n",
    "        rows.append((sid, name, region, city, is_active, maybe_inconsistent_date(last_updated), maybe_inconsistent_date(created_at)))\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"store_id\", T.StringType(), False),\n",
    "        T.StructField(\"store_name\", T.StringType(), True),\n",
    "        T.StructField(\"region\", T.StringType(), True),\n",
    "        T.StructField(\"city\", T.StringType(), True),\n",
    "        T.StructField(\"is_active\", T.IntegerType(), True),\n",
    "        T.StructField(\"last_updated\", T.StringType(), True),\n",
    "        T.StructField(\"created_at\", T.StringType(), True),\n",
    "    ])\n",
    "    return spark.createDataFrame(rows, schema)\n",
    "\n",
    "def gen_products(n):\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        pid = new_uuid()\n",
    "        created_at = random_ts_days_ago(1500)\n",
    "        last_updated = created_at\n",
    "        is_active = 1 if _r.random() > 0.03 else 0\n",
    "        name = inject_encoding(_r.choice([\"Laptop\",\"Smartphone\",\"Headphones\",\"Monitor\",\"Keyboard\",\"Mouse\",\"Printer\",\"Webcam\",\"Speaker\",\"VR Headset\",\"SSD\",\"HDD\"])) + f\" {_r.randint(100,999)}\"\n",
    "        brand = _r.choice(BRANDS)\n",
    "        category = _r.choice(CATEGORIES)\n",
    "        list_price = round(_r.uniform(9.99, 3499.0), 2)\n",
    "        cost = round(list_price * _r.uniform(0.4, 0.85), 2)\n",
    "        currency = _r.choice(CURRENCIES)\n",
    "        rows.append((pid, name, brand, category, cost, list_price, currency, is_active, maybe_inconsistent_date(last_updated), maybe_inconsistent_date(created_at)))\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"product_id\", T.StringType(), False),\n",
    "        T.StructField(\"product_name\", T.StringType(), True),\n",
    "        T.StructField(\"brand\", T.StringType(), True),\n",
    "        T.StructField(\"category\", T.StringType(), True),\n",
    "        T.StructField(\"unit_cost\", T.DoubleType(), True),\n",
    "        T.StructField(\"list_price\", T.DoubleType(), True),\n",
    "        T.StructField(\"currency\", T.StringType(), True),\n",
    "        T.StructField(\"is_active\", T.IntegerType(), True),\n",
    "        T.StructField(\"last_updated\", T.StringType(), True),\n",
    "        T.StructField(\"created_at\", T.StringType(), True),\n",
    "    ])\n",
    "    return spark.createDataFrame(rows, schema)\n",
    "\n",
    "def gen_employees(n, store_df):\n",
    "    sids = [r.store_id for r in store_df.select(\"store_id\").collect()]\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        eid = new_uuid()\n",
    "        store = _r.choice(sids)\n",
    "        hire = random_ts_days_ago(2000)\n",
    "        last_updated = hire\n",
    "        term_flag = 1 if _r.random() < 0.07 else 0\n",
    "        term_date = random_ts_days_ago(300) if term_flag else None\n",
    "        is_active = 0 if term_flag else 1\n",
    "        first = _r.choice([\"Asha\",\"Bengt\",\"Chen\",\"Deepa\",\"Eva\",\"Farid\",\"Greta\",\"Hiro\",\"Isha\",\"Jonas\",\"Kiran\",\"Lars\"])\n",
    "        last  = _r.choice([\"Lind\",\"Nguyen\",\"Olofsson\",\"Persson\",\"Quinn\",\"Rahman\",\"Svensson\",\"Tanaka\",\"Ulrich\",\"Vega\",\"Wang\",\"Xu\"])\n",
    "        name = inject_encoding(f\"{first} {last}\")\n",
    "        role = _r.choice([\"Cashier\",\"Manager\",\"Sales Associate\",\"Stock Clerk\",\"HR\",\"Security\",\"Cleaner\",\"Barista\",\"Technician\",\"Supervisor\"])\n",
    "        salary = round(_r.uniform(24000, 95000), 2)\n",
    "        rows.append((eid, name, store, role, salary, is_active, maybe_inconsistent_date(last_updated), maybe_inconsistent_date(hire), maybe_inconsistent_date(term_date) if term_date else None))\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"employee_id\", T.StringType(), False),\n",
    "        T.StructField(\"employee_name\", T.StringType(), True),\n",
    "        T.StructField(\"store_id\", T.StringType(), True),\n",
    "        T.StructField(\"role\", T.StringType(), True),\n",
    "        T.StructField(\"monthly_salary\", T.DoubleType(), True),\n",
    "        T.StructField(\"is_active\", T.IntegerType(), True),\n",
    "        T.StructField(\"last_updated\", T.StringType(), True),\n",
    "        T.StructField(\"hire_date\", T.StringType(), True),\n",
    "        T.StructField(\"termination_date\", T.StringType(), True),\n",
    "    ])\n",
    "    return spark.createDataFrame(rows, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74cb7bc7-5c1c-46b9-9754-bfd5acc939fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbe3957-1ead-40e6-b91b-49ebbbf36a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_customers, num_stores, num_products, num_employees = 5000, 40, 1500, 600\n",
    "customers_df = gen_customers(num_customers)\n",
    "stores_df    = gen_stores(num_stores)\n",
    "products_df  = gen_products(num_products)\n",
    "employees_df = gen_employees(num_employees, stores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c048b3f-bb66-44cc-9ece-d4c3a36d2dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Saving data to partioned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd01897a-f1b8-4d0c-b62a-92445c50ca03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dt = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "for name, df in [(\"customers\", customers_df),(\"stores\", stores_df),(\"products\", products_df),(\"employees\", employees_df)]:\n",
    "    (df.withColumn(\"ingestion_date\", F.lit(load_dt)).write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_date\").save(PATHS[name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "872c7d18-c2c5-43b6-9af0-2472e746e626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introducing Updates \n",
    "## Product updates\n",
    "### Suffles rows randomly and select top 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bed351c-21be-46f2-936e-d7d920ac3a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Suffles rows randomly and give 1000 random records \n",
    "num_updates = 200\n",
    "pids = [r.product_id for r in spark.read.format(\"delta\").load(PATHS[\"products\"]).orderBy(F.rand()).select(\"product_id\").limit(1000).collect()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2824eb7-074f-4798-8d24-0addbea9379a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare json with induced updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabbfe0c-11b7-47fb-9840-a72c1457ffdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "updates = []\n",
    "for _ in range(num_updates):\n",
    "    now_s = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "    r = random.random()\n",
    "    if r < 0.70 and pids:\n",
    "        pid = random.choice(pids)\n",
    "        updates.append({\"event_type\":\"UPDATE\",\"product_id\":pid,\"list_price\":round(random.uniform(5,3999),2),\"currency\":random.choice(CURRENCIES),\"is_active\":1,\"last_updated\":now_s})\n",
    "    elif r < 0.85:\n",
    "        pid = new_uuid()\n",
    "        updates.append({\"event_type\":\"CREATE\",\"product_id\":pid,\"product_name\":inject_encoding(random.choice([\"Router\",\"Dock\",\"Microphone\",\"Camera\"]))+f\" {random.randint(100,999)}\",\"brand\":random.choice(BRANDS),\"category\":random.choice(CATEGORIES),\"unit_cost\":round(random.uniform(3,1200),2),\"list_price\":round(random.uniform(9,3999),2),\"currency\":random.choice(CURRENCIES),\"is_active\":1,\"created_at\":now_s,\"last_updated\":now_s})\n",
    "    else:\n",
    "        pid = random.choice(pids)\n",
    "        updates.append({\"event_type\":\"SOFT_DELETE\",\"product_id\":pid,\"is_active\":0,\"last_updated\":now_s})\n",
    "\n",
    "upd_df = spark.createDataFrame(updates, \"event_type string, product_id string, product_name string, brand string, category string, unit_cost double, list_price double, currency string, is_active int, created_at string, last_updated string\")\n",
    "# .coalesce : used to write all data in single file.\n",
    "upd_df.coalesce(1).write.mode(\"append\").json(PATHS[\"api_products_updates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f69075b7-afb9-4046-8c5c-ea47fd5bc256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- SFTP Simulation: CSV inventory snapshots ---\n",
    "store_ids = [r.store_id for r in stores_df.select(\"store_id\").collect()]\n",
    "prod_ids  = [r.product_id for r in products_df.select(\"product_id\").limit(600).collect()]\n",
    "rows = []\n",
    "now_iso = datetime.utcnow().isoformat(timespec=\"seconds\")\n",
    "for sid in store_ids:\n",
    "    for pid in random.sample(prod_ids, k=min(120, len(prod_ids))):\n",
    "        qty = max(0, int(random.gauss(40, 20)))\n",
    "        if random.random() < 0.02:\n",
    "            qty = int(qty * random.uniform(5, 20))\n",
    "        unit_cost = round(random.uniform(1, 1200), 2)\n",
    "        pid_val = pid if random.random() > 0.01 else \"\"\n",
    "        sid_val = sid if random.random() > 0.01 else None\n",
    "        rows.append((sid_val, pid_val, qty, unit_cost, now_iso))\n",
    "inv_df = spark.createDataFrame(rows, \"store_id string, product_id string, on_hand int, unit_cost double, snapshot_ts string\")\n",
    "inv_df.coalesce(1).write.mode(\"append\").option(\"header\",\"true\").csv(PATHS[\"sftp_inventory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7305991-1721-484a-a231-5554b5a54c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Ingest inventory CSV to bronze ---\n",
    "bronze_inv = (spark.read.option(\"header\",\"true\").csv(PATHS[\"sftp_inventory\"]).withColumn(\"ingestion_date\", F.to_date(F.current_timestamp())))\n",
    "bronze_inv.write.mode(\"append\").format(\"delta\").partitionBy(\"ingestion_date\").save(PATHS[\"inventory\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ee2b39-973b-4351-aead-3ba13fe32fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Event Hubs (Kafka) config\n",
    "EVENT_HUBS_NAMESPACE = 'mockdata' # conf[\"event_hubs_namespace\"]\n",
    "EVENT_HUBS_NAME = 'retailsales' #conf[\"event_hubs_name\"]\n",
    "EVENT_HUBS_CONNSTR = dbutils.secrets.get(\"retail-scope\", \"eventhubsconn\")  # secure\n",
    "KAFKA_BROKER = f\"{EVENT_HUBS_NAMESPACE}.servicebus.windows.net:9093\"\n",
    "KAFKA_OPTIONS = {\n",
    "  \"kafka.bootstrap.servers\": KAFKA_BROKER,\n",
    "  \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "  \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "  \"kafka.request.timeout.ms\": \"60000\",\n",
    "  \"kafka.session.timeout.ms\": \"30000\",\n",
    "  \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{EVENT_HUBS_CONNSTR}\";'\n",
    "}\n",
    "KAFKA_TOPIC = EVENT_HUBS_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be35c435-42ee-4332-ba87-150afb63d5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b475c7-4984-4d71-b5ed-4cdb0f3ffd70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Streaming sales to Event Hubs + bronze archive ---\n",
    "customers_b = customers_df.select(\"customer_id\").distinct()\n",
    "stores_b    = stores_df.select(\"store_id\").distinct()\n",
    "products_b  = products_df.select(\"product_id\", \"list_price\")\n",
    "\n",
    "cids = [r.customer_id for r in customers_b.limit(2000).collect()]\n",
    "sids = [r.store_id for r in stores_b.collect()]\n",
    "prows = [(r.product_id, float(r.list_price)) for r in products_b.limit(1500).collect()]\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def udf_rand_cust():\n",
    "    import random\n",
    "    return random.choice(cids) if cids else str(uuid.uuid4())\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def udf_rand_store():\n",
    "    import random\n",
    "    return random.choice(sids) if sids else str(uuid.uuid4())\n",
    "\n",
    "@F.udf(\"struct<product_id:string, unit_price:double>\")\n",
    "def udf_rand_product():\n",
    "    import random\n",
    "    if prows:\n",
    "        pid, price = random.choice(prows)\n",
    "        price = float(price) if price else round(random.uniform(5, 3999),2)\n",
    "        return {\"product_id\": pid, \"unit_price\": price}\n",
    "    return {\"product_id\": str(uuid.uuid4()), \"unit_price\": round(random.uniform(5, 3999),2)}\n",
    "\n",
    "base_stream = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 20).load()\n",
    "    .withColumn(\"event_ts\", F.current_timestamp())\n",
    "    .withColumn(\"order_id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"customer_id\", udf_rand_cust())\n",
    "    .withColumn(\"store_id\", udf_rand_store())\n",
    "    .withColumn(\"ps\", udf_rand_product())\n",
    "    .withColumn(\"product_id\", F.col(\"ps.product_id\"))\n",
    "    .withColumn(\"unit_price\", F.col(\"ps.unit_price\"))\n",
    "    .drop(\"ps\")\n",
    "    .withColumn(\"quantity\", (F.rand()*5 + 1).cast(\"int\"))\n",
    "    .withColumn(\"currency\", udf_random_currency())\n",
    "    .withColumn(\"gross_amount\", F.col(\"unit_price\")*F.col(\"quantity\"))\n",
    "    .withColumn(\"event_time_str\", F.when(F.rand()<0.12, F.date_format(F.col(\"event_ts\"), \"dd/MM/yyyy HH:mm:ss\")).otherwise(F.date_format(F.col(\"event_ts\"), \"yyyy-MM-dd HH:mm:ss\")))\n",
    ")\n",
    "\n",
    "sales_for_kafka = base_stream.select(F.to_json(F.struct(\"order_id\",\"event_time_str\",\"customer_id\",\"store_id\",\"product_id\",\"quantity\",\"unit_price\",\"currency\",\"gross_amount\")).alias(\"value\"))\n",
    "\n",
    "kafka_query = (sales_for_kafka.writeStream.format(\"kafka\").options(**KAFKA_OPTIONS).option(\"topic\", KAFKA_TOPIC).outputMode(\"append\").option(\"checkpointLocation\", f\"{PATHS['sales']}/_chk_kafka\").start())\n",
    "bronze_query = (base_stream.withColumn(\"ingestion_date\", F.to_date(F.current_timestamp())).writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\", f\"{PATHS['sales']}/_chk_bronze\").start(PATHS[\"sales\"]))\n",
    "\n",
    "\n",
    "print(\"Streaming started. Use 'kafka_query.stop(); bronze_query.stop()' in a separate cell to stop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be23dd59-2bd3-44e3-96c4-375b6f4ea29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kafka_query.stop(); bronze_query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1406552276165346,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "hybrid_retail_mock_core_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}